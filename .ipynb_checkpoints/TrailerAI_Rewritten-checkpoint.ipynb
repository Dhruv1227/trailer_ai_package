{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfac Trailer AI \u2014 Rewritten Notebook\n",
        "End-to-end: CSV \u2192 Download (yt_dlp API) \u2192 Features \u2192 Quick Train \u2192 Evaluate \u2192 Trailer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q yt-dlp opencv-python librosa==0.10.1 numpy==1.24.4 pandas tqdm scikit-learn joblib matplotlib python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, re, json, shutil, subprocess\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import numpy as np, pandas as pd\n",
        "import cv2, librosa, yt_dlp\n",
        "import matplotlib; matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "WORKDIR = os.path.join(os.getcwd(), 'data')\n",
        "for d in ('raw','features','models','out'): os.makedirs(os.path.join(WORKDIR,d), exist_ok=True)\n",
        "print('Working dir:', WORKDIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def basename_noext(p: str) -> str:\n",
        "    return os.path.splitext(os.path.basename(p))[0]\n",
        "def seconds_from_vtt_ts(ts: str) -> float:\n",
        "    parts = re.split(r'[,:.]', ts); h,m,s = int(parts[0]), int(parts[1]), float(parts[2]); return h*3600+m*60+s\n",
        "def parse_vtt(vtt_path: str):\n",
        "    if not vtt_path or not os.path.exists(vtt_path): return []\n",
        "    entries=[]; block=[]\n",
        "    with open(vtt_path,'r',encoding='utf-8',errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line=line.rstrip('\\n')\n",
        "            if not line.strip():\n",
        "                if block:\n",
        "                    for i,ln in enumerate(block):\n",
        "                        if '-->' in ln:\n",
        "                            t1,t2=[x.strip() for x in ln.split('-->')]\n",
        "                            try:\n",
        "                                s=seconds_from_vtt_ts(t1); e=seconds_from_vtt_ts(t2.split(' ')[0]); text=' '.join(block[i+1:]).strip()\n",
        "                                if e>s: entries.append((s,e,text))\n",
        "                            except: pass\n",
        "                            break\n",
        "                block=[]\n",
        "            else:\n",
        "                block.append(line)\n",
        "    return entries\n",
        "def caption_overlap(captions, start, end):\n",
        "    if not captions: return 0.0\n",
        "    dur=max(1e-6,end-start); covered=0.0\n",
        "    for s,e,_ in captions:\n",
        "        inter=max(0.0,min(end,e)-max(start,s)); covered+=inter\n",
        "    return min(covered,dur)/dur\n",
        "def caption_keyword_density(captions,start,end):\n",
        "    import re; tot=0.0; dur=max(1e-6,end-start)\n",
        "    for s,e,text in captions:\n",
        "        inter=max(0.0,min(end,e)-max(start,s))\n",
        "        if inter>0:\n",
        "            w=len(re.findall(r'\\w+',text.lower())); tot+=w*(inter/(e-s+1e-6))\n",
        "    return float(tot/dur)\n",
        "def normalize(vals):\n",
        "    a=np.asarray(vals,dtype=np.float32); \n",
        "    if a.size==0: return a\n",
        "    mn,mx=float(np.min(a)),float(np.max(a)); \n",
        "    return np.zeros_like(a) if mx-mn<1e-12 else (a-mn)/(mx-mn)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def download_video(url: str, raw_dir: str, write_subs: bool=False, cookies_path: Optional[str]=None,\n",
        "                   sleep_requests: Optional[int]=10, max_sleep_interval: Optional[int]=20):\n",
        "    os.makedirs(raw_dir, exist_ok=True)\n",
        "    ydl_opts = {\n",
        "        'outtmpl': os.path.join(raw_dir, '%(id)s.%(ext)s'),\n",
        "        'format': 'mp4/bestvideo[ext=mp4]+bestaudio[ext=m4a]/best',\n",
        "        'merge_output_format': 'mp4',\n",
        "        'noplaylist': True,\n",
        "        'quiet': True,\n",
        "        'retries': 3,\n",
        "    }\n",
        "    if write_subs:\n",
        "        ydl_opts.update({'writesubtitles': True,'writeautomaticsub': True,'subtitleslangs': ['en'],'subtitlesformat':'vtt'})\n",
        "    if cookies_path and os.path.exists(cookies_path): ydl_opts['cookiefile']=cookies_path\n",
        "    if sleep_requests and max_sleep_interval:\n",
        "        ydl_opts['sleep_interval_requests']=sleep_requests; ydl_opts['max_sleep_interval_requests']=max_sleep_interval\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        if info is None: raise RuntimeError('yt_dlp failed to fetch info')\n",
        "        if 'requested_downloads' in info and info['requested_downloads']:\n",
        "            filepath = info['requested_downloads'][0]['filepath']\n",
        "        else:\n",
        "            vid = info.get('id'); ext = info.get('ext','mp4'); filepath = os.path.join(raw_dir, f\"{vid}.{ext}\")\n",
        "    vtt_path=None; vid=os.path.splitext(os.path.basename(filepath))[0]\n",
        "    for cand in (os.path.join(raw_dir,f\"{vid}.en.vtt\"), os.path.join(raw_dir,f\"{vid}.vtt\")):\n",
        "        if os.path.exists(cand): vtt_path=cand; break\n",
        "    return filepath, vtt_path\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    video_id: str; start: float; end: float; motion: float=0.0; audio: float=0.0; cap_overlap: float=0.0; kw_density: float=0.0; score: float=0.0\n",
        "def sample_video_histograms(mp4_path: str, fps_sample: float = 2.0):\n",
        "    cap=cv2.VideoCapture(mp4_path)\n",
        "    if not cap.isOpened(): raise RuntimeError(f'Cannot open video: {mp4_path}')\n",
        "    fps=cap.get(cv2.CAP_PROP_FPS) or 30.0; step=max(1,int(round(fps/fps_sample)))\n",
        "    ts_list, diffs, prev_hist=[], [], None; frame_idx=0\n",
        "    while True:\n",
        "        ret=cap.grab();\n",
        "        if not ret: break\n",
        "        if frame_idx % step == 0:\n",
        "            ret,frame=cap.retrieve(); \n",
        "            if not ret: break\n",
        "            ts=cap.get(cv2.CAP_PROP_POS_MSEC)/1000.0\n",
        "            hsv=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "            h0=cv2.calcHist([hsv],[0],None,[64],[0,180])\n",
        "            h1=cv2.calcHist([hsv],[1],None,[64],[0,256])\n",
        "            h2=cv2.calcHist([hsv],[2],None,[64],[0,256])\n",
        "            hist=np.concatenate([h0.flatten(),h1.flatten(),h2.flatten()]).astype(np.float32); hist/= (np.sum(hist)+1e-6)\n",
        "            diffs.append(0.0 if prev_hist is None else float(np.sum(np.abs(hist-prev_hist))))\n",
        "            prev_hist=hist; ts_list.append(ts)\n",
        "        frame_idx+=1\n",
        "    cap.release(); ts=np.array(ts_list,dtype=np.float32); diffs=np.array(diffs,dtype=np.float32); return ts,diffs\n",
        "def detect_scenes(ts, diffs, thresh: float = 0.55):\n",
        "    if len(ts)==0: return [(0.0,0.0)]\n",
        "    cuts=[0];\n",
        "    for i in range(1,len(diffs)):\n",
        "        if diffs[i]>thresh: cuts.append(i)\n",
        "    cuts.append(len(ts)-1)\n",
        "    bounds=[]\n",
        "    for i in range(len(cuts)-1):\n",
        "        s=float(ts[cuts[i]]); e=float(ts[cuts[i+1]]); \n",
        "        if e>s: bounds.append((s,e))\n",
        "    return bounds\n",
        "def make_chunks(bounds: List[Tuple[float,float]], min_len=2.0, max_len=6.0, video_id='vid'):\n",
        "    chunks=[]\n",
        "    for s,e in bounds:\n",
        "        cur=s\n",
        "        while cur + min_len <= e:\n",
        "            end=min(cur+max_len,e); chunks.append(Chunk(video_id=video_id,start=cur,end=end)); cur+=min_len\n",
        "    return chunks\n",
        "def avg_motion(diffs, ts, start, end):\n",
        "    mask=(ts>=start)&(ts<=end)\n",
        "    return float(np.mean(diffs[mask])) if np.any(mask) else 0.0\n",
        "def audio_rms(mp4_path: str, start: float, end: float):\n",
        "    dur=max(0.0,end-start)\n",
        "    if dur<=0.0: return 0.0\n",
        "    try:\n",
        "        y,sr=librosa.load(mp4_path, sr=None, offset=max(0.0,start), duration=dur)\n",
        "        return float(np.sqrt(np.mean(y**2))) if y.size>0 else 0.0\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "def compute_features_for_video(mp4_path: str, vtt_path: Optional[str], min_seg: float, max_seg: float, scene_thresh: float):\n",
        "    video_id=basename_noext(mp4_path); captions=parse_vtt(vtt_path) if vtt_path else []\n",
        "    ts,diffs=sample_video_histograms(mp4_path, fps_sample=2.0)\n",
        "    if len(ts)==0: raise RuntimeError('Failed to sample frames.')\n",
        "    bounds=detect_scenes(ts,diffs,thresh=scene_thresh) or [(0.0,float(ts[-1]))]\n",
        "    chunks=make_chunks(bounds,min_len=min_seg,max_len=max_seg,video_id=video_id)\n",
        "    for c in tqdm(chunks, desc=f'Features {video_id}'):\n",
        "        c.motion=avg_motion(diffs,ts,c.start,c.end)\n",
        "        c.audio=audio_rms(mp4_path,c.start,c.end)\n",
        "        c.cap_overlap=caption_overlap(captions,c.start,c.end) if captions else 0.0\n",
        "        c.kw_density=caption_keyword_density(captions,c.start,c.end) if captions else 0.0\n",
        "    m_n=normalize([c.motion for c in chunks]); a_n=normalize([c.audio for c in chunks]); t_n=normalize([0.5*c.cap_overlap+0.5*c.kw_density for c in chunks])\n",
        "    for i,c in enumerate(chunks): c.score=float(0.4*m_n[i]+0.4*a_n[i]+0.2*t_n[i])\n",
        "    return chunks\n",
        "def greedy_select(chunks, target_len, min_gap):\n",
        "    chosen=[]; used=[]; total=0.0\n",
        "    for c in sorted(chunks,key=lambda x:x.score, reverse=True):\n",
        "        if total>=target_len*0.98: break\n",
        "        if any(abs(c.start-s)<min_gap for s in used): continue\n",
        "        dur=c.end-c.start\n",
        "        if total+dur>target_len+2.0: continue\n",
        "        chosen.append(c); used.append(c.start); total+=dur\n",
        "    return chosen\n",
        "def render_trailer(mp4_path, chunks, out_mp4, target_len, min_seg):\n",
        "    selected=greedy_select(chunks,target_len=target_len,min_gap=min_seg/2.0)\n",
        "    if not selected: raise RuntimeError('No chunks selected for trailer.')\n",
        "    tmp=os.path.join(os.path.dirname(out_mp4),'_tmp'); os.makedirs(tmp, exist_ok=True)\n",
        "    parts=[]\n",
        "    for i,c in enumerate(selected):\n",
        "        part=os.path.join(tmp,f'part_{i:03d}.mp4')\n",
        "        cmd=['ffmpeg','-y','-ss',f'{c.start:.3f}','-to',f'{c.end:.3f}','-i',mp4_path,'-c:v','libx264','-preset','veryfast','-crf','23','-c:a','aac','-b:a','128k',part]\n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError('ffmpeg is required on PATH. Please install ffmpeg.') from e\n",
        "        parts.append(part)\n",
        "    fl=os.path.join(tmp,'files.txt')\n",
        "    with open(fl,'w',encoding='utf-8') as f:\n",
        "        for p in parts: f.write(f\"file '{os.path.abspath(p)}'\\n\")\n",
        "    os.makedirs(os.path.dirname(out_mp4), exist_ok=True)\n",
        "    subprocess.run(['ffmpeg','-y','-safe','0','-f','concat','-i',fl,'-c','copy', out_mp4], check=True)\n",
        "    shutil.rmtree(tmp, ignore_errors=True); print('\ud83c\udfac Trailer saved:', out_mp4)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "csv_path=os.path.join(WORKDIR,'video_ids.csv')\n",
        "if not os.path.exists(csv_path):\n",
        "    pd.DataFrame({'video_id':['dQw4w9WgXcQ']}).to_csv(csv_path, index=False)\n",
        "df=pd.read_csv(csv_path)\n",
        "if 'video_id' not in df.columns: raise ValueError(\"CSV must have a 'video_id' column.\")\n",
        "urls=[f\"https://www.youtube.com/watch?v={vid}\" for vid in df['video_id'].dropna().astype(str).tolist()]\n",
        "print(f'\u2705 Loaded {len(urls)} video(s):')\n",
        "for u in urls: print('  ', u)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "min_seg, max_seg, scene_thresh = 2.0, 6.0, 0.55\n",
        "raw_dir=os.path.join(WORKDIR,'raw'); features_dir=os.path.join(WORKDIR,'features')\n",
        "WRITE_SUBS=False; COOKIES_PATH=None\n",
        "for url in tqdm(urls):\n",
        "    try:\n",
        "        mp4,vtt=download_video(url, raw_dir, write_subs=WRITE_SUBS, cookies_path=COOKIES_PATH)\n",
        "        chunks=compute_features_for_video(mp4, vtt, min_seg, max_seg, scene_thresh)\n",
        "        out_csv=os.path.join(features_dir, f\"{basename_noext(mp4)}.csv\")\n",
        "        pd.DataFrame([asdict(c) for c in chunks]).to_csv(out_csv, index=False)\n",
        "        print('\u2705 Saved features \u2192', out_csv)\n",
        "    except Exception as e:\n",
        "        print('\u26a0\ufe0f Error on URL:', url, '\u2192', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def label_chunks_from_annotations(df: pd.DataFrame, ann_csv: str) -> pd.DataFrame:\n",
        "    ann=pd.read_csv(ann_csv)\n",
        "    req={'video_id','start_sec','end_sec'}\n",
        "    if not (set(ann.columns) >= req): raise RuntimeError('annotations.csv must have columns: video_id,start_sec,end_sec')\n",
        "    def iou(a_s,a_e,b_s,b_e):\n",
        "        inter=max(0.0, min(a_e,b_e)-max(a_s,b_s)); union=(a_e-a_s)+(b_e-b_s)-inter; return inter/union if union>0 else 0.0\n",
        "    labels=[]\n",
        "    for _,r in df.iterrows():\n",
        "        v=ann[ann.video_id==r['video_id']]; lbl=0\n",
        "        for _,a in v.iterrows():\n",
        "            if iou(r['start'], r['end'], float(a['start_sec']), float(a['end_sec'])) >= 0.5: lbl=1; break\n",
        "        labels.append(lbl)\n",
        "    out=df.copy(); out['label']=labels; return out\n",
        "def train_quick_model(features_dir: str, ann_csv: str, model_path: str):\n",
        "    files=[os.path.join(features_dir,f) for f in os.listdir(features_dir) if f.endswith('.csv')]\n",
        "    if not files: raise RuntimeError('No feature CSVs found. Run extraction first.')\n",
        "    X=pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "    X=label_chunks_from_annotations(X, ann_csv)\n",
        "    feat_cols=['motion','audio','cap_overlap','kw_density']; y=X['label'].values.astype(np.int32)\n",
        "    clf=LogisticRegression(max_iter=1000, class_weight='balanced'); clf.fit(X[feat_cols].values, y)\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True); joblib.dump(clf, model_path)\n",
        "    print('\u2705 Quick model saved to', model_path)\n",
        "ann_csv=os.path.join(WORKDIR,'annotations.csv')\n",
        "if not os.path.exists(ann_csv):\n",
        "    with open(ann_csv,'w') as f:\n",
        "        f.write('video_id,start_sec,end_sec\\n')\n",
        "        vids=sorted([basename_noext(p) for p in os.listdir(raw_dir) if p.endswith('.mp4')])\n",
        "        if vids:\n",
        "            f.write(f\"{vids[0]},10,16\\n\")\n",
        "model_path=os.path.join(WORKDIR,'models','ranking_model.pkl')\n",
        "train_quick_model(features_dir, ann_csv, model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _rank_metrics_per_video(labels, scores, ks=(5,10)):\n",
        "    labels=np.asarray(labels,dtype=np.int32); scores=np.asarray(scores,dtype=np.float32)\n",
        "    order=np.argsort(-scores); y=labels[order]; npos=int(y.sum()); res={}\n",
        "    for K in ks:\n",
        "        K=int(min(K,len(y))); topk=y[:K]; tp=int(topk.sum())\n",
        "        prec=tp/max(1,K); rec=tp/max(1,npos) if npos>0 else 0.0\n",
        "        f1=0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "        gains=(2**topk-1)/np.log2(np.arange(2,K+2)); dcg=gains.sum()\n",
        "        ideal=np.sort(labels)[::-1][:K]; idcg=((2**ideal-1)/np.log2(np.arange(2,len(ideal)+2))).sum(); ndcg=(dcg/idcg) if idcg>0 else 0.0\n",
        "        res[f'P@{K}']=prec; res[f'R@{K}']=rec; res[f'F1@{K}']=f1; res[f'NDCG@{K}']=ndcg\n",
        "    ap=0.0\n",
        "    if npos>0:\n",
        "        hits=0; precs=[]\n",
        "        for i,rel in enumerate(y, start=1):\n",
        "            if rel==1: hits+=1; precs.append(hits/i)\n",
        "        ap=float(np.mean(precs)) if precs else 0.0\n",
        "    res['MAP']=ap; return res\n",
        "def evaluate_model(features_dir, annotations_csv, model_path, out_csv, out_png, ks=(5,10)):\n",
        "    files=[os.path.join(features_dir,f) for f in os.listdir(features_dir) if f.endswith('.csv')]\n",
        "    if not files: raise RuntimeError('No feature CSVs found.')\n",
        "    df=pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "    df=label_chunks_from_annotations(df, annotations_csv)\n",
        "    feat_cols=['motion','audio','cap_overlap','kw_density']\n",
        "    clf=joblib.load(model_path)\n",
        "    scores=None\n",
        "    if hasattr(clf,'predict_proba'):\n",
        "        try:\n",
        "            proba=clf.predict_proba(df[feat_cols].values)\n",
        "            scores=proba[:,1] if proba.ndim==2 and proba.shape[1]>=2 else proba.ravel()\n",
        "        except Exception:\n",
        "            scores=None\n",
        "    if scores is None: scores=clf.predict(df[feat_cols].values)\n",
        "    df['pred']=scores\n",
        "    vids=sorted(df['video_id'].unique().tolist()); rows=[]\n",
        "    agg={f'P@{k}':0.0 for k in ks}; agg.update({f'R@{k}':0.0 for k in ks}); agg.update({f'F1@{k}':0.0 for k in ks}); agg.update({f'NDCG@{k}':0.0 for k in ks}); agg['MAP']=0.0\n",
        "    n=0\n",
        "    for vid in vids:\n",
        "        sub=df[df.video_id==vid]; m=_rank_metrics_per_video(sub['label'].values, sub['pred'].values, ks=ks)\n",
        "        rows.append({'video_id':vid, **m})\n",
        "        for k in ks:\n",
        "            agg[f'P@{k}']+=m[f'P@{k}']; agg[f'R@{k}']+=m[f'R@{k}']; agg[f'F1@{k}']+=m[f'F1@{k}']; agg[f'NDCG@{k}']+=m[f'NDCG@{k}']\n",
        "        agg['MAP']+=m['MAP']; n+=1\n",
        "    if n>0:\n",
        "        for k in ks:\n",
        "            agg[f'P@{k}']/=n; agg[f'R@{k}']/=n; agg[f'F1@{k}']/=n; agg[f'NDCG@{k}']/=n\n",
        "        agg['MAP']/=n\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok=True); pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
        "    labels=[*(f'P@{k}' for k in ks), *(f'R@{k}' for k in ks), *(f'F1@{k}' for k in ks), *(f'NDCG@{k}' for k in ks), 'MAP']\n",
        "    values=[agg[x] for x in labels]\n",
        "    plt.figure(figsize=(10,4)); plt.bar(labels, values); plt.ylim(0,1.0); plt.title('Ranking Model Evaluation'); plt.ylabel('Score'); plt.tight_layout()\n",
        "    os.makedirs(os.path.dirname(out_png), exist_ok=True); plt.savefig(out_png); plt.close(); return agg\n",
        "eval_csv=os.path.join(WORKDIR,'models','eval_report.csv'); eval_png=os.path.join(WORKDIR,'models','eval_report.png')\n",
        "metrics=evaluate_model(features_dir, ann_csv, model_path, eval_csv, eval_png, ks=(5,10)); metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp4=sorted([p for p in os.listdir(raw_dir) if p.endswith('.mp4')])[-1]\n",
        "mp4_path=os.path.join(raw_dir, mp4)\n",
        "chunks=compute_features_for_video(mp4_path, None, 2.0, 6.0, 0.55)\n",
        "clf=joblib.load(model_path)\n",
        "Xf=np.array([[c.motion,c.audio,c.cap_overlap,c.kw_density] for c in chunks], dtype=np.float32)\n",
        "scores=None\n",
        "if hasattr(clf,'predict_proba'):\n",
        "    try:\n",
        "        proba=clf.predict_proba(Xf); scores=proba[:,1] if proba.ndim==2 and proba.shape[1]>=2 else proba.ravel()\n",
        "    except Exception: scores=None\n",
        "if scores is None: scores=clf.predict(Xf)\n",
        "mn,mx=float(np.min(scores)), float(np.max(scores)); p=(scores-mn)/(mx-mn+1e-9)\n",
        "for i,c in enumerate(chunks): c.score=float(p[i])\n",
        "out_mp4=os.path.join(WORKDIR,'out', f\"{basename_noext(mp4_path)}_trailer.mp4\")\n",
        "render_trailer(mp4_path, chunks, out_mp4, 45, 2.0)\n",
        "out_mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import Image, Video, display\n",
        "display(Image(os.path.join(WORKDIR,'models','eval_report.png')))\n",
        "Video(out_mp4, embed=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}